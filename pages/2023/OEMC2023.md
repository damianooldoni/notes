---
Title: Open-Earth Monitor Global Workshop 2023

---

## General

- Website: https://earthmonitor.org/gw2023/
- Program: [pdf](https://earthmonitor.org/wp-content/uploads/2023/10/OEMC-GW-Program_LV_PDF-version-1.pdf), [web](https://pretalx.earthmonitor.org/gw2023/schedule/)
- Organizer: [Open Earth Monitor](Open-Earth-Monitor project) project 
- Host: [EURAC research](https://www.eurac.edu/en), Bolzano, Italy
- Founder: European Commision via Horizon-Europe program

## Notes

### Reprocudible and Reusable Remote sensing workflows

Authors: **Edzer Pebesma**, Anna Anghelea
[Abstract](https://pretalx.earthmonitor.org/gw2023/talk/VDFCUV/)

Reproducible remote sensing papers: Edzer is trying to launch a new open access and free (no publicatoin fee) journal about remote sensing.

What's "easy" to do?
- Package versioning
- System requirements versioning, 
- Operating system (Virtual Machine or container)

On the other hand, what's difficult to do?
- Run workflows on large datasets. Possible solution: share smaller datasets and argue why that is sufficient
- Using [Google Eearth Engine](https://earthengine.google.com/), arcGIS, [ERDAS](https://hexagon.com/products/erdas-imagine), but also HPC dedicate models: to what extent are such workflows reproducbile? Are we still doing open science then?
- Get someone to review your code and give feedback:
	- journals have no review requirements, neither for authores nor for reviewers
	- [Code check](codecheck.org.uk): would you like to run my code, review it and give me feedback? It's a nice initiative: Independent execution of computations underlying research articles. You can add a badge on GItHub in green if somebody run successfully your code.
	- When submitting a manuscript, report on successult reproduction / code review


a key motivation behind openEO was the ability to compare cloud based platforms with each other (tech validation)
What's hard: how do we describe, store, advertise, share, and find [openEO process graphs](https://api.openeo.org/v/0.3.0/processgraphs/)? The json file you get it's quite difficult to read/understand. 
Should we share process graphs or rather Jupyter / R-markdown / Quarto notebooks? These are easier to read and can be a mix of text and code.

ESA initiatives:
	-launching the Open Science persistent demonstrator co-sponsored with NASA and implemented by [OGC](https://www.ogc.org/) to build interoperability across platforms and demonostrate reporducilibity of workflows
	- promoting the process graph to promote reproducibility
	- coordination of several communities working on related technologies including geo-datacubes and datacube APIs to facilitate reproducibility
	- expanding and advancing the Deep Earth System Data Lab [Deep ESDL](https://www.earthsystemdatalab.net/) that is based fully on open source technology
	- new activity called earth-code ([general link](https://eo4society.esa.int/2023/06/27/invitation-to-tender-earth-code-earth-science-collaborative-open-development-environment/) info, [details]( https://esastar-publication-ext.sso.esa.int/ESATenderActions/details/61267)) starting in November 2023 will have dedicated work stream: e.g. by "how to discover and execute workflows?" "How to maintain a healthy community that practices open Earth System science", "how to persistently store science data AND code AND documentation from Earth Science projects following FAIR principles?"

Idea of Pebesma: make the reproducibility work a publication on its own.

To reproduce things, containerization and software distribution is absolutely needed: new initiative mentioned from audience: [geonix](https://github.com/imincik/geonix), A cross-platform geospatial software distribution and development environment.

## COPERNICUS DATA SPACE ECOSYSTEM: IMMEDIATELY AVAILABLE DATA AND ASSOCIATED SERVICES

Jędrzej Bojanowski

[Abstract](https://pretalx.earthmonitor.org/gw2023/talk/ELM7EN/)

Unfortunately I could not follow it: I was in another parallel session. 

Some key points from the abstract: Copernicus Data Space Ecosystem (CDSE) represents a key milestone in access to Copernicus satellite data. Not only, but data of other satellites are available, e.g. Landsat, SMOS and Envisat. Moreover, CDSE will offer access to commercial satellite data.

The user does not have to order and wait for the data anymore.

Direct access also allows bulk data processing and streaming, e.g. via OGC services (WMS/WFS)

Another novelty of CDSE are the various interfaces where the data are available:
- old-fashioned download
- OData, an ESA-adopted standard, based on RESTful API
- STAC catalogue and STAC API that become a standard in the EO community and being onboarded to OGC
- Jupyter Hub, very suitable tool for prototyping, developing, and testing applications open-source, online, interactive web application which gives access to computational environments and resources without burdening the users with installation and maintenance tasks

The vast majority of the described capabilities are available free-of-charge for the individual's use.




## European platforms to support the Green Deal

Authors: **Benjamin Schumacher** (EODC), Christian Briese

[Abstract](https://pretalx.earthmonitor.org/gw2023/talk/LXPPYZ/).


EODC: located in Wien. Active in the C-GLOPS (Copernicus - global land), C3S2 (Copernicus Climate Change Service), Lot 4 (land hydrology and cryosphere) and Global FLood Monitoring Service (fast analysis and predictions for use of public authorities)

EODC is composed of:
- cloud
- storage
- backup
- HPC

EO platforms show: 
- fragmentation, redundancy and lacking coordination among each other
- prevalence of old Virtual Machine

Idea: openEO API (an Horizon 2020 project) which was part of the openEO Platform. openeo.cloud is a combination of several backends, which are a federation of data access via the [SpatioTemporal Asset Catalog](https://stacspec.org/en/about/), STAC.

The end user consults the federated catalogue STAC and behind the screen STAC will do for you the data request to the right data provider such as [cesnet](https://www.cesnet.cz/e-infrastruktura-3/data-storage/?lang=en), [terra scope](https://terrascope.be/en), [eodc](https://eodc.eu/about-us/), [SINERGISE](https://www.sinergise.com/)

KEY MESSAGE: federation of data catalogue.

Another important stuff: community that builds a framework for FAIR scientific computing and data access


storage nodes -> NPC / Cloud Compute (the compute nodes) -> Jupyter notebooks (users)

[C-SCALE](https://c-scale.eu/) project helps to reach a federation of data catalogue. Notice that C-SCALE will be integrated with the [European Open Science Cloud](https://eosc-portal.eu/) (EOSC) so that C-SCALE solutions can be seamlessly integrated in all the other EOSC-supported research and innovation processes and practices.

C-SCALE mission is clear: C-SCALE  will enhance EOSC Portal with pan-European federated data and computing infrastructure services for Copernicus.

What's the green deal data space: a federation of data ecosystems to jointly tackle climate change
The European Commision laws concerning the green deal data space:
- Single Market for data: [COM/2020/66](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52020DC0066)
- Tackling climate and environmental: [COM/2019/640](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2019%3A640%3AFIN)


So, this is the road to follow:
- Abstracting complexity by federated data catalogue
- Provide transparency by open science (and FAIR I would add) principles
- Pixel to contintental scalability by scalable buliding block processes

The most important observation from the Q&A: the federation of data is not really a problem, we will implement it with STAC. The **challenge** is **federating** the analysis **code**: implicitly it means that all partners will support openEO standards and its versions.


## DestinE's Core Service Platform
Inés Sanw Morére

Destination Earth (DestinE) intiative
DESP Destination Earth services platform
DestinE develop highly accurate digital model of the Earth -> monitor, simulate and predict
Goal: assist users to adaptation strategies and mitigation measures

destination-earth.eu

Objectives: contribute to policy makers, researchers (provide access to data, allow modelling including Machine Learning) and general public (raising awareness)

We are in Phase 1: 2022 2024: progressive entering into operation of routine services

System implementation

Notice there is one sysmte developed by the 3 entities

Users interface the COre Service Platform of ESA wich is connected to the Digital Twin Engine of ECMWF and the Data Lake of EUMETSAT

DestinE's Core Service PLatform (DESP) is the access point for useres to DestinE system

DESP is a comprehensive framework for the analysis and manaement
enablse the development and exploitation of applications and services targeting DestinE data
direct access to users

Core services are reached. The DESOP Advanced services are planned to be ready in Dec 2023, a.o. collaboration

serco is the main contractor together with other software companies csuch as aliaspace CGI ThalesAleniaSpace deimos OVHcloud
Idea is to get validation for gradual opening in Dec 23.


DESP Services aims to provide an open and inclusive access to DESP users and commit to standards of quality and performance for operations

Upcominb events:
webinars and participations to:
- Second Destination Earth User eXchange in Bonn (Germany) in 13-14 November 2023
- BiDS BIG DATA FROM SPACE 2023 (6-9 November 2023) in Vienna (Austria)

Available datasets: still to be defined

There are already two open ecosystems: how this extra one will work with the others?
There will be collaboration as esa is involved in the development and it's almost the sae



## Analysis Ready Data (ARD)

Peter Strobl


Once upon a time in EO: data must to become operable (data access, calibration, filtering, ...)
Interoperability: ability of EXCHANGE information and USE the exchanged information

There are two groups in [CEOS](https://ceos.org/) where interoperability is involved.

CEOS interoperability factors
Semantics: naming stuff
architecture: organisational structure of concepts, process and assets
Interfacing: data exchange protocols and applications
QUality
Policy


Current + in deve CEOS -ARD specifications: ceos.org/ard/index.html#specs
CEOS has a key role to play in defining ARG for the community

OGC ARD SWG was approced may 3 2023
ISO has designated the ARD series of standards to be ISO 19176
19176-1 is the first Part

semantics is challenging and very important
examples: EO is not remote sensing
downscaling vs upscaling
And als about the meaning of some terms like "Observations", "in-situ"

proposal for improving the situation:
consistent and based on a firm coundation of base terms
hierarchical
authoritative (all majos stakeholers IEEE, CEOS, ISO TC211) commit to use the same vocabulary
Sustainatble commit resource sto support the project for 5-10 ys
open

and now let's speak about Architecture aspects

it's a lineary sequence in which one level builds on the previous one
raw data -> level 1 (reformatting calibation)- > correction orthorectification 'level2' - harmonisation resampling (regridding) (level 3) fusion (level 4)

It's important to keep track about all these steps otherwise we don't know anymore what we are speaking about as the output could be quite different than the raw data.



Numbers are used to identify Measurant steps
Letters indicate spatio(temporal) Geometry steps

LO raw data
L1A calibration ready data
L1B orthorectification ready data
L2C conflation/coimbination ready data
L3B fusion ready data
L3C/D analysis mdoel ready data
L44CD/D inference ready information


interoperability in the architecture meaning

Inteoperability means a can not as  is and 

ARD are meant to build data cubes
OGC data cube community practire says: all layers in a da cube need to share the smae grid to allow interoperability bteween layers
CO-gridding is an imoprtant elemnt within the data cube

BUT two data cubes (or ARDd datasets) do not need to share the same grid to be considered inteoperable (?)
If so, interoperability restricted to a "one way "road? -> a cube can only be involved once during an anamysis workflow
Or we need resmapling: is this still interoperabilility?

We need then:
- point cloud approach

good practice ins spatial discretisation of Earth surface:
specific: based on ellipsoidal Earth model
intrinsic: the grid is a project

WMTS standard base EPSG 3857: good but no everywhere

the MGRS standard (base UTM EPSG:326xx): 

Idea: the EQUI17 grid (TU Vienna) 
Other interesting ideas: Cube based DGGS: no discontinuities, no gaps, no overlaps, hierarchical nesting, cell indexing, but moderate distortions and most of all no EPSG

Problem is how to discrtise the earth's surface?
square? but then you don't cover it all
then: choose a refinement ratio (divie by 2 or by 3?) and define clearly the hierarchy

Which road to take: continuous (point-clouds) or dicrete (gtrids)

THere will not be one ARD as there are many types of userse, data and analysis
the way we strcucture of ARD will impact how interoperability will emerge
succes of ARD depens on whether its architecutre is future proof


there is a open discussion in GitHub. Not links mentioned in slides, maybe contact the speaker: Peter.Strobl@ec.europa.eu

question: data cubes are qtucally created with a workflow in mind, so they are in some ways specific and users typically don't want to deal with points or not gridded data.





## Open Earth Climate

Joaquin van Peborgh from OpenEarth Foundation

acronyms: GHG: 

Agrgegtaing nested emissions data in Open Vlimate and what's next

OpenEarth builds open source digital products and infrastructure

Check openclimate.network to see how dashboard works. At a certain level you will find where data become not available anymore (the endpoint of open data)

in digs.openearth.org: you can see that there is no coordination between national and provincial data (national emission is lower than the sum of the data provided by provinces)
Notice that digs.openearth has a Python wrapper to get the data

Open Climate data model is oper source data model to track climate action across non-state actors
Integrating 67 data srouces
360k+ emission records
there is an open API for all this data (check GitHub) and a python client

Cities and regions lack tehcnical capacity fo decarbosie even if thery are some of the most critical climate acotrs
data collection and lack of internal capacity arethe main pain point
it takes 4 persons and 6 ponts to collect the data needed and complete an inventory
Limited internal capacity: you need trained analysts and knowledge is lost when they leave
Heterogenous and messy data
lack of streamlined tools

So, the initiative CityCatalyst started!
It provides ready to yse data, intuitive UI& AI support startign with GHG inventories
drastically reduce efforts and cotss in data collection for local governments, e.g; via dta templates and inventories

data also accessable through an open API (work in progress): see workshop tomorrow "Using spatial data to build GHG inventories"




## GEO knowledge Hub to preserve and share EO APPLICATIONS

Paola De Salvo, Felipe Carlos

All the materials used are on github.com/geo-knowledge-hub/geo-knowledge-hub-workshop

General website: https://gkhub.earthobservations.org/

GEO (group on Earht Observations) is a consortium of 110 partners
GEO data sharing principles and Open Knowledge Statement docuemnts

Download the GEO WOrk programme 2023-2015

GEO KNOWLEDGE HUB is a digital library

GEO shares/Preserves and thel to find materials from the EO acpplications

To do this there is a Knowledge Package

It contains in-situ data, tools, articles & notes, user stories

Main categories: 
	- dataset 
	- publication 
	- software 
	- other

To upload a package:
 deinfe metdata
  upload files
  link external resources
  assing DOI

  Example from the Digital Earth AFRICA: in the knowledge package you have a page with metadata with auxiliary data and files and executable notebok and description documents. Example: [Monitoring Mangrove Extent Data](https://gkhub.earthobservations.org/records/5ph8r-4k650)


Community is key: without community of uers and knowledge resource providers, nothing is possible
At the moment 4 communities such as the Digital Earth AFRICA

There is a thematic search functionality, a space search functionality
Communities are welcome: a group of users

Users could require training and GEO people can help to provide such training sessions.

Real time feedback from the users is possible (see [feed homepage](gkhub.earthobservations.org/feed))

gkub.earthobservations.org 

What is the roadmap? started as a prototype, since the last year it works in the feature extensions and contacting users. The GEO secretariate gave green light, but it's not known how far this will be financed in the future. Probably yes.

The Knowledge package creation page is very similar to zenodo.
DOIs assigned to both the knowledge package and each of its resources.

Once you have a knowledge package you can create one or more resources associated to it

A REST API is also present

Using the REST API you can use workflows available, load data, get data

Important: open source platform! 
ALl the tools behind are open: INVENIORDM (it will be the new base of Zenodo),OpenSearch, PostgreSQL, Python, React, Flask

Infrastructure: Amazon Elastic Compute CLoud; Amazon SImplet Storage Service

At the moment after one year:
108 package, 531 resources


## Let's co-create the Green Deal Data Space

[Joan Masó Pau](https://www.creaf.cat/staff/joan-maso-pau) involved in the [AD4GD](https://ad4gd.eu/)


Define: what a data space. It's well define by the IDSA international data space association

IDS standard enables data exchange betwee a dertified data provider and a recipient based on mutually agred tules
Data space are therefore not open by default.

How to do it in practice? the softwaredevelopers will speak about:
Identity provider (who is who)
IDS Connectors: allows the exchange of data via the data endpoints (key point of everything)
Data apps: small proc tasks, e.g. transform, clearn or analyse data
Medatadata Broker: registration, publication, maintenance and query of connectore descriptions
Clearing House: loggin service
Vocabulary Hub
standardized terms to describe data, services, contracts etc


On IDSA you have docker images: https://github.com/International-Data-Spaces-Association/DataspaceConnector/pkgs/container/dataspace-connector



BUT is it needed while working with open data? NO. Geospatial services, APIs and Open Nodes do not require connectors. We setup INSPIRE without connectors

The European  Commission from one side talks about stimulating the difital economy in a 
make better use of publicly held data for reserch for the common good and support voluntary data

How many data spaces will be? Hopefully one, sinble Euroepan data space. But it's not easy

We have silo's...

Challenge is enormous: the green dealdata space is the most heterogeneous and mutidisciplniary of all
We als oneed common OGC Standards and APIs

How to reconcile OGC standards with IDSA approach?

OGC and the IDSA sign a Memorandum of Understanding; this means that probably they will work together to contribute to the creation 

Personal proposal of Joan: adding a node connector under test (CUT) to link the user and Open Node under test (ONU) to link ??


Research in some solutions to breaksilods in the Grean Deal Data Space:
Project AD4GD: it aims to share and combine among others gridded data in datacubes (coverage data) in collaboration with B-cubed and FAIRiCUBE. Also testin OGC web services and in the GGC web APIs with IDSA connectors


## Machine Learning tools and systems support for EO data processing and applications

link abstract: https://pretalx.earthmonitor.org/gw2023/talk/BHQ9JG/

[EO4EU](https://eo4eu.eu/) project

Platform intelligence: machine larning orkflows and their components
systems support: data processign and storage
challgnes: complex data requiring complex processing to obtain meaningful outputs. And scaling! So, it aims to reduce annotation effort in downstream apps and tries to develop efficient learnt compression algorithms. See below 

EO4EU provides or aims to provide an innovative ML-based learned compression algorithms to enhance accessibility of the data sources and so reducing the footprints of storage capacity and the network bandwidth requriements

EO4EU control and core plane components of the platform whcih allows scalability

SO, you have an orchestration Language which deals with three blocks:
EO sources -> EO4EU processin infrastructure (fusion, semantic annotation, ..) -> EO4EU applications (Dashboards, API, data processing workflows, XR)


This design architecture seems in some way similar to LifeWatch ERIC infrastructure

Data Tier

a set of data sources is the input of the platform. Heterogeneous data that need preprocessing with the help of a Knowledge Graph.

Examples Data Sources:
- interlink heterogeneouws data srouces (different formats) with the EO4EU ecosystem through Open Apis
- access to historical and daily EO datasets


Data Tier:

Knowledge Graph-based Decision Making

After the Data Tier we have a ML Tier which provides all ML models in a toolbox for hte post processing of the retrieved data.

Front-endTier

Provides multi-dimensional User Itnerface (Webn XR, CLI, API) that enables the users to consume the results.

The visualization dashboard allows you to visualize the results ina configurable way based on a workflow editor.

You can control the workflow via a Work Flow Editor which is a design and configuration tool. The components are presented as nodes.

There are 7 [pilot use cases](https://eo4eu.eu/use-cases), a.o. forest ecosystems, soil erosion, environmental pests, ocean monitoring


pre-processor -> fusion Engine -> ML engine -> FaaS (function as a service) function -> front-end user interface


About Fusion Engine: using kafka which produces a yaml which is use by [Kubeflow](https://www.kubeflow.org/) which returns a message to next component

A fusion model is published in the AI/ML Marketplace with some attributes (metadata):
- icon
- name
- what input
- what output
- functionality description


Annotation efficient learning:
- deep learning models are data hungry
- provision of annotations is a significant bottleneck
- unsupervised learning in particular [Self-Supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning) (SSL) can help to solve the bottleneck


SSL is supervised but most of the job is done via unsupervised learning:

> SSL belongs to supervised learning methods insofar as the goal is to generate a classified output from the input. At the same time, however, it does not require the explicit use of labeled input-output pairs. Instead, correlations, metadata embedded in the data, or domain knowledge present in the input are implicitly and autonomously extracted from the data. These supervisory signals, generated from the data, can then be used for training.

Source: ["What is Self-Supervised Learning? | Will machines ever be able to learn like humans?](https://medium.com/what-is-artificial-intelligence/what-is-self-supervised-learning-will-machines-be-able-to-learn-like-humans-d9160f40cdd1)

SSL is growing fast even in remote sensing

Example providing the Contrastive self-supervised learning. Sentinel-2 data are used.

After the unsupervised learning is done, supervised learning starts so using a smaller Convolutional neural network. This will be given as feedback to the unsupervised learning for fine-tuning.

Check performance at training with 0.1%, 1% or 10% of annotated instances against the golden standard performance with 100% of annotated instances

Conclusions:

With extremely low annotation budget SSL performs better than standard supervised learning
Important: a linear classifier over the SSL learnt representation does extremely well in the low regime
At 10% of annotated dat astandard supervised learning starts having an advantage, approaching the golden standard.

Another important aspect 

EO data are produced at 100TBs/day rates making data compresssion a cental tool in EO data management


How to solve this?

By Variational Autoencoders (VAEs), which is a model based on inference and generation (from input X to Z to the reconstructed X). Check this interesting [blogpost](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) about VAE

One way to do this is the Vector Quantized VAE (VQ-VAE) maps the continuous latent space (images patches) to a discrete latent space (codewords).
Check this interesting [blogpost](https://shashank7-iitd.medium.com/understanding-vector-quantized-variational-autoencoders-vq-vae-323d710a888a) explaining VQ-VAE


## Build and visualize your own raster data cube

Leandro Parente, Murat Sahin

[Abstract](https://pretalx.earthmonitor.org/gw2023/talk/SWJG3T/)

I couldn't follow the workshop due to sessions running in parallel.

Materials:
- slides ([Gslides](https://docs.google.com/presentation/d/1BVC7jI186hec-447v1ZOL8B6HwcDbWyeTpeva_cJJNs) shared by author)
- computation notebooks in [gitHub](https://github.com/scikit-map/scikit-map/blob/master/docs/notebooks/01-raster-data.ipynb) and [colab](https://colab.research.google.com/drive/1jRUFoRyk3JV1dTtMzTdBi6rnBff_VNje)


## OpenEO in action. Learn how to get started with openEO via Web Editor, Python and R. 

Peter Zellner and Michele Claus
Abstract: https://pretalx.earthmonitor.org/gw2023/talk/ZYJF7M/

Workshop not followed due to session running in parallel. However, it should be very interesting to get a look to the online material:

- CDSE [homepage](https://dataspace.copernicus.eu/)
- Cubes & Clouds GitHub [repo](https://github.com/EO-College/cubes-and-clouds)


## Open LandMap: Global ARCO environmental layers

Tom Hengl

Abstract: https://pretalx.earthmonitor.org/gw2023/talk/3SF9JG/

Not followed unfortunately due to session running in parallel.

Shortly: How to access the OpenLandMap STAC, how to further use and extend layers, how to prepare layers for OpenLandMap and test them locally, and then also include in the interface / web-hosting. Some examples in R and Python are shown. More on GitHub [spatial-layers](https://github.com/openlandmap/spatial-layers/) repo.

References:
- Hengl, T., Collins, T.N., Wheeler, I. & MacMillan, R.A. (2019, March 27). Everybody has a right to know what’s happening with the planet: towards a global commons. Medium (Towards Data Science), http://doi.org/10.5281/zenodo.2611127
- Hengl, T., Walsh, M. G., Sanderman, J., Wheeler, I., Harrison, S. P., & Prentice, I. C. (2018). Global mapping of potential natural vegetation: an assessment of machine learning algorithms for estimating land potential. PeerJ, 6, e5457. https://doi.org/10.7717/peerj.5457

Quote of the day: Quote of the day "Julia language: program like Python, run like C++" 
https://github.com/JuliaDataCubes/YAXArrays.jl
Back-end solution: https://github.com/meggart/DiskArrayEngine.jl


2023-10-06

## Monitoring deforestation in AMazonia: transitioning from visual interpretation to satellite time series analysis

Gilberto Camara

Visual interpretation by experts is hard for ML to beat.

premises 1: Basic question is "what's there?" Take observations.
premises 2: we use observations to learn the world
premises 3: we use words to express things
premises 4: Our statements are ture or false depending on thether thepy correspond to the facts in the real world

So, we ascribe meaning to nature
names = sense + reference

Do you process space first and then compare across time? typically yes.
But sometimes it's bteer to dothe opposite: timeseries indicate change! E.g. take timeseries and then compare across land cover catagories

That's the origin of the [SITS]( Satellite Image Time Series Analysis on Earth Observation Data Cubes) workflow and its R package, [sits](https://e-sensing.github.io/sitsbook/).

So, SITS generates maps. Shown the map os SITS map of 2022. The challenge for ML is: beat that!

Getting now on ML: there are many ways of partitionin EO data
Is there an objective truth when working with ML methods? Not really. 

Vapnik says: in a complex world one should give up explainability to gain a better predicatability. 

COnclusions:
1. EO for compliance: high accuracy is crucial
2. No objective truth in ML for EO: training classes are context-dependent: No high accuracy for global data, no way you can achieve that.
3. Time-first, space later: key to measure deforestation and LUCC


Question of Edzer Pebesma: why not processing space and time together instead of choosing a dimension first?
Answer: it's working in progress. Still, the point is that we need still to end up with comparison between time series of land cover categories.

Another question: why not using training data? I am afraid that we could not yet get away of loosing a lot of time to get good training data. Deep learning needs a lot of training dataset.



## Global land use and land cover monitoring: Data to impact

abstract: https://pretalx.earthmonitor.org/gw2023/talk/8M7MXV/

Lindsey Sloat, from the World Reources Institute, [Land & Carbon Lab](https://www.landcarbonlab.org/) (LCL)

[Global Forest Watch](https://www.globalforestwatch.org/: there is an early alert tool to get a notification when deforestation is occurring at a place you are interested to. It's also to get alerts when fires occurr.

6.7 milion people monitoring
91 million hecatres of forest monitored

Evolution of the tool: it eplxoded immediately reaching a "Peak of Inflated Expectations" followed by a deflation as tech couldn't follow the hugh amount of users or the high expectation of the users, among others policy makers. A sustainable growth followed up to today.

Global Pasture Watch: ampping and monitoring global grasslands and livestock (iDiv, Cornell University, Lapig, GLAD, IIASA and Open GeoHub are involved)

monitoring conversion free complaice
restoration plannincg
land sectore greenhouse gas accounting

It was developed a qgis pulgin called fast grid inspection (FGI)

EO / grided data  -> reference samples -> ML and feedback back to further fine tuning. Making data and models are open. Capacity building is important as well, giving workshops and training sessions.

 Something else: do you know that this exists? near real time vegetation change for all lands! HLS Dis-Alerts

Based on Hybrid Landsat Sentinel-2. 

It builds also a Disturbance alerts with range of confidence 0-100%

Check LCL website!

All data of LCL will be available via API!


## Monitoring Biodiversity in Agricultural Supply Chain: an Irish Case Study

Cian White

Abstract: https://pretalx.earthmonitor.org/gw2023/talk/QSQKW7/

Method used to anlyaze pictures: Object Based Image nalysis using the Falwenswalb algorithm



## Predicting the future Earth under climate scenarios: an R tutorial

Carmelo Bonannella

Abstract: https://pretalx.earthmonitor.org/gw2023/talk/KHQVSS/


A representative concentration Pathway (RCP) is a creehouse gas concerntation trejectory by the IPCC

Four pathways were used for climate modeling and reserach for the IPCC 5th Assessemment Report, but three were used in the study

RCP | Temperature increase (Celsius) |
--- | --- |
RCP 2.6 | 1.0
RCP 4.5 | 1.8 Celsius
RCP 8.5 | 3.7


The IUCN is mostlyknown for the IUCN Red List of Threatened Species

Biome: biogeographical unit

T1- Tropical/subtropical forest biome
T4 - Svannas and grasslands biome
T5 Deserts and semi-deserts biome

The question is: what will be the **impact** of climate change in these **biomes?

Reference / training data: BIOME 6000 dataset: 8959 observations indicating pollen/fossil reconstruction of ecosystems dated 6000 years ago

It's a high quality dataset (see Hengl et al. 2018 to know more about the dataset and its preparation). Extended with pseudopoints e.g. in Brazil where we had no points at all.

Number of observations is not equal across biomes

Data-driven / ML Approach basd on the points in the dataset. Output of the model is shown on openlandmap viewer which shows how the Earth would like if human impact not existed (BIOME dataset refers to 6000 years ago).

Lookup table is needed to translate BIOME dataset classes to IUCN classes of course.

Now, you need to predict the future. To do so, you look to he past.

Bioclim dataset from CHELSA
- 17 bioclimatic variables derived from monthly temperature and rainfall value
- 54 climatic variables
- used as long-term trends seasonality and extreme or limitin environmental factors
- 1x1 km resolution

In this way we could use these feature predictions to predict how the biomes will look like.
WHat will be the impact of the changes of these 17 bioclimatic variables to the biomes?


Topographic variables from MERIT DEM (Yamazaki et al. 2017): 6 topographic variables indicating elevation, slopte, aspect and other DTM derivatives

THe work is publishedo on zenodo: Current and future global distribution of potential biomes under climate change scenarios

140 class probability layers + uncertinty layers
7 hard class from BIOME 6000 layers + margin of victory layers
7 hard class from IUCN layers + margin of victory layers (difference between "winning" biome is high/low in comparison to the second more likely biome to occurr in that cell)


[PNVmaps](https://github.com/Envirometrix/PNVmaps) is a repo on GitHub to reproduce the same analysis.

It contains an important [tutorial](https://github.com/Envirometrix/PNVmaps/tree/master/tutorial).

They dis some cross-validation.


Plotting the results:
interesting the map where:
- in red are all areas that will change according to our model with a margin of victory value greater than 50%
- in yellow are all the areas with margin of victory values less than 50%

Some of these changes have already been observed! 

future plans/ work in progress:
- improve resolution to 30m
- increase time cover period


## Using spatial data to build a nested climate accounting network

Joaquin Van Peborgh

Abstract: https://pretalx.earthmonitor.org/gw2023/talk/KMZSRR/

Useful links:
- access the [platform](https://openclimate.network/)
- [GitHub](https://github.com/Open-Earth-Foundation/OpenClimate)
- [Data Schema](https://github.com/Open-Earth-Foundation/OpenClimate-Schema)

This workshop brings us back to its presentation (add crss refernce) where he stressed that cities/local entities represent 70% of the global emissions, but only 5% of them have GHG inventories.

Why?

- Political problems: sometimes multiple departments, change of governements and so change of priorites
- Practical problem: time consumming, so difficult to have dedicated and trained staff to collect and process questionaries

In the presentations was stressed that they can provde help via templates to collect their own data, or way to get data from third party providers to habe **good enough data**. These data needs to be useful for downstream needs

Idea is to 
 1. get data from Earth Observations data
 2. use some tools as Climate TRACE Carbon Mapper Forest & Soil C, Urban Canopy Google EIE or others
 3. Visualize results

 Which sectors? Stationary energy, waste, transport, industrail, agriculture


 [Climate TRACE](https://climatetrace.org/)
 Climate TRACE SOlid Waste methodology: a ML model for estimatig solid water emissions on a global scale by aggragting data from diverse source. Data are collected from different sources.

 Deep dive: Gridding EDGAR data to assign to city boundaries
 Some transformations need to be done as data have a limited resolution.

 https://bit.ly/GHGI: itneractive tool which didn't work well as everybody was tired.
